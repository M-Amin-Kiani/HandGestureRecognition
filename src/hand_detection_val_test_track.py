# -*- coding: utf-8 -*-
"""Hand_Detection_Val_Test_Track.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eYiME8Ivb5thOLyomVq9CzKPmP7y2_YO
"""

from ultralytics import YOLO

# # Load a model
model = YOLO("final_model.pt")

validation_results = model.val(data="C:\\Users\\Lenovo\\Desktop\\data\\data.yaml", imgsz=640, batch=16, iou=0.6, device="cpu")

from ultralytics import YOLO

# # Load a model
model = YOLO("final_model.pt")
results = model.predict('E:\\2\\2804_color.png' , conf = .2)
# Process results list
for result in results:
    boxes = result.boxes  # Boxes object for bounding box outputs
    masks = result.masks  # Masks object for segmentation masks outputs
    keypoints = result.keypoints  # Keypoints object for pose outputs
    probs = result.probs  # Probs object for classification outputs
    obb = result.obb  # Oriented boxes object for OBB outputs
    result.show()  # display to screen
    result.save(filename="result.jpg")  # save to disk

import cv2

from ultralytics import YOLO
model = YOLO("final_model.pt")
# Load the YOLO11 model
# model = YOLO("yolo11n.pt")

# Open the video file
video_path = "f.mp4"
cap = cv2.VideoCapture(0)

# Loop through the video frames
while cap.isOpened():
    # Read a frame from the video
    success, frame = cap.read()

    if success:
        # Run YOLO11 tracking on the frame, persisting tracks between frames
        results = model.track(frame )

        # Visualize the results on the frame
        annotated_frame = results[0].plot()

        # Display the annotated frame
        cv2.imshow("model Tracking", annotated_frame)

        # Break the loop if 'q' is pressed
        if cv2.waitKey(1) & 0xFF == ord("q"):
            break
    else:
        # Break the loop if the end of the video is reached
        break

# Release the video capture object and close the display window
cap.release()
cv2.destroyAllWindows()